{
 "metadata": {
  "name": "",
  "signature": "sha256:6fb264bb691e0504dcaaafc5a98a81ad5a5d443c5e115da8782f27aad8b05375"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have more data than you could fit into the memory of your machine, chances are that you would want to use something different than `fit` as `fit` needs to load the data into memory once to train the classifier. However, some of the classifiers  support `partial_fit` in Scikit-Learn which does not require data to load into memory. You could train you data in batches so that you could process it as you fit the data into the memory of the machine. Further, if you want to train further your classifier with new data, you do not have to retrain the whole classifier, but rather feed the new data into the classifier by calling `partial_fit` function. By doing so, you have a way of improving the classifier with available new data rather than going through loading the data once into the memory with the old data that you trained the classifier already."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My suggestion is that you would divide the data into batches in such a way that size of the batches would be close to the memory of the machine as there is an overhead of function call to model. The smaller you call `partial_fit` is the faster you train the model. In these cases, efficiency is not just bound to the data size but how you divide the data into batches. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Get the banking data that Amazon uses"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}