{
 "metadata": {
  "name": "",
  "signature": "sha256:2b48f184f63e1686ab1eb4d2f83b5bdc41efa671fb7e1cfbb9e0ffc2e66985e5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generalization\n",
      "- How to generalize better?\n",
      "- What can we do to make the machine learning model generalizes on the test data as well?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model selection is the task of choosing best performing classifier out of a candidate set of classifiers which is very useful for libraries like Scikit-Learn as the api for different parameters stay across different classifiers. That makes it very easy to try different \"estimators\" in the same dataset. \n",
      "\n",
      "After learning about parameter search, one could do parameter search on a given classifier, but in practice you rarely depend on one classifier. Ideally, you need to compare different classifiers (as their learning function differs a lot), their performance on your real dataset may vary drastically. In order to choose not best paremeters for a given classifier, one needs to look at different classifiers and then do a parameters search on top of the those multiple classifiers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model selection makes sure that not only I will get the best parameters but also best classifier for a given candidate of classifiers for a scoring function that I will optimize for. But this is not good enough because I want to also see the effectof total number of features as well; when I change the total number of features, how does it affect the score of a classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import csv\n",
      "import itertools\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "from sklearn import cross_validation\n",
      "from sklearn import ensemble\n",
      "from sklearn.feature_extraction import text\n",
      "from sklearn import feature_extraction\n",
      "from sklearn import feature_selection\n",
      "from sklearn import linear_model\n",
      "from sklearn import metrics\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import svm\n",
      "from sklearn import tree\n",
      "\n",
      "plt.style.use('fivethirtyeight')\n",
      "\n",
      "_DATA_DIR = 'data'\n",
      "_NYT_DATA_PATH = os.path.join(_DATA_DIR, 'nyt_title_data.csv')\n",
      "\n",
      "_PLT_LEGEND_OPTIONS = dict(loc=\"upper center\", \n",
      "                           bbox_to_anchor=(0.5, -0.15),\n",
      "                           fancybox=True, \n",
      "                           shadow=True, \n",
      "                           ncol=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colors = [ii.strip() for ii in '#30a2da, #fc4f30, #e5ae38, #6d904f, #8b8b8b'.split(',')]\n",
      "colors += ['#' + ii.strip() for ii in '348ABD, A60628, 7A68A6, 467821,D55E00,  CC79A7, 56B4E9, 009E73, F0E442, 0072B2'.split(',')]\n",
      "markers = itertools.cycle([\"o\", \"D\"])\n",
      "colors = itertools.cycle(colors)\n",
      "\n",
      "def cv(X, y, clf, nfeats, clfname, scoring=metrics.accuracy_score, n_folds=10):\n",
      "  stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds)\n",
      "  accuracy, ii = 0., 0\n",
      "  for train, test in stratified_k_fold:\n",
      "    ii += 1\n",
      "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
      "    clf.fit(X_train, y_train)\n",
      "    y_pred = clf.predict(X_test)\n",
      "    score = scoring(y_test, y_pred)\n",
      "    accuracy += score\n",
      "  accuracy /= float(n_folds)\n",
      "  return accuracy\n",
      "\n",
      "def plot_accuracies(accuracies, xvals, legends):\n",
      "  fig = plt.figure(figsize=(16, 12))\n",
      "  ax = fig.add_subplot(111)\n",
      "  for ii in range(0, accuracies.shape[0]):\n",
      "    ax.plot(xvals, accuracies[ii, :], color=next(colors), marker=next(markers), label=legends[ii])\n",
      "  plt.xlabel(\"Number of Features\")\n",
      "  plt.ylabel(\"Accuracy\")\n",
      "  plt.title(\"Accuracy vs Number of Features\")\n",
      "  ax.set_xscale(\"log\")\n",
      "  box = ax.get_position()\n",
      "  ax.set_position([box.x0, box.y0 + box.height * 0.3, box.width, box.height * 0.7])\n",
      "  ax.legend(**_PLT_LEGEND_OPTIONS)\n",
      "  plt.show()\n",
      "\n",
      "def estimator_name(clf):\n",
      "  return type(clf).__name__\n",
      "    \n",
      "def select_model(X, y, scoring=metrics.accuracy_score):\n",
      "  n_features = np.array([10, 100, 500, 1000, 5000, 10000, 20000, 50000, 100000])\n",
      "  clfs = [\n",
      "    naive_bayes.BernoulliNB(),\n",
      "    naive_bayes.MultinomialNB(),\n",
      "    naive_bayes.GaussianNB(),\n",
      "    tree.DecisionTreeClassifier(),\n",
      "    ensemble.RandomForestClassifier(n_estimators=10),\n",
      "    svm.LinearSVC(random_state=0),\n",
      "    linear_model.LogisticRegression(),\n",
      "    linear_model.SGDClassifier(),\n",
      "    linear_model.RidgeClassifier(),\n",
      "  ]\n",
      "  \n",
      "  classifier_names = map(estimator_name, clfs)\n",
      "  feature_selection_methods = [feature_selection.f_classif]\n",
      "  accuracies = np.zeros((len(clfs), len(n_features), len(feature_selection_methods)))\n",
      "  for kk in range(len(feature_selection_methods)):\n",
      "    X_feature_selected = X.copy().toarray()\n",
      "    for jj in range(len(n_features)):\n",
      "      for ii in range(len(clfs)):\n",
      "        accuracies[ii, jj, kk] = cv(X_feature_selected, y, clfs[ii], n_features[jj], classifier_names[ii], scoring=scoring)\n",
      "  for k in range(len(feature_selection_methods)):\n",
      "    for i in range(len(clfs)):\n",
      "      print \"%22s \" % classifier_names[i],\n",
      "      for j in range(accuracies.shape[1]):\n",
      "        print \"%5.3f\" % accuracies[i, j, k],\n",
      "      print\n",
      "    plot_accuracies(accuracies[:, :, k], n_features, classifier_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(_NYT_DATA_PATH) as nyt:\n",
      "    nyt_data = []\n",
      "    nyt_labels = []\n",
      "    csv_reader = csv.reader(nyt)\n",
      "    for line in csv_reader:\n",
      "      nyt_labels.append(int(line[0]))\n",
      "      nyt_data.append(line[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Unigram & Bigram for features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array([''.join(el) for el in nyt_data])\n",
      "y = np.array([el for el in nyt_labels])\n",
      "\n",
      "\n",
      "vectorizer = text.TfidfVectorizer(min_df=2, \n",
      "                                  ngram_range=(1, 2), \n",
      "                                  stop_words='english', \n",
      "                                  strip_accents='unicode', \n",
      "                                  norm='l2')\n",
      " \n",
      "example = unicode(X[5])\n",
      "\n",
      "print(\"Example string:      {}\".format(example))\n",
      "print(\"Preprocessed string: {}\".format(vectorizer.build_preprocessor()(example)))\n",
      "print(\"Tokenized string:    {}\".format(str(vectorizer.build_tokenizer()(example))))\n",
      "print(\"N-gram data string:  {}\".format(str(vectorizer.build_analyzer()(example))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string:      Clinton Defends His Policies In Kosovo, China and Mexico\n",
        "Preprocessed string: clinton defends his policies in kosovo, china and mexico\n",
        "Tokenized string:    [u'Clinton', u'Defends', u'His', u'Policies', u'In', u'Kosovo', u'China', u'and', u'Mexico']\n",
        "N-gram data string:  [u'clinton', u'defends', u'policies', u'kosovo', u'china', u'mexico', u'clinton defends', u'defends policies', u'policies kosovo', u'kosovo china', u'china mexico']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In TfidfVectorizer, one may choose the `ngram_range`, in this example, it is bigram and unigram. Choosing bigram enables us to capture phrases that could play an important role in determining the class of the observation in the dataset. If you believe, even a longer sequence of words `internet of things` play an important role, you could pass `ngram_range` to `(1, 3)` to include trigrams as well. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array(['Dole Courts Democrats', 'Yanks End Drought; Mets Fall in Opener',\n",
        "       'Lumpectomies Seen As Equal in Benefit To Breast Removals', ...,\n",
        "       'Delays Hurting U.S. Rebuilding In Afghanistan',\n",
        "       'SENATE APPROVES $1 BILLION TO AID COLOMBIA MILITARY',\n",
        "       'POLITICS: THE MONEY; A Hollywood Production: Political Money'], \n",
        "      dtype='|S127')"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<2161x2171 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 10377 stored elements in Compressed Sparse Row format>"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_model(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot not only shows the relative strength of classifiers with respect to each for a given number of features but also provides a good picture on the effect of number of features for the classification task at the hand. Generally even if Scikit-Learn has all grid search other capabilities to output the best estimator for a given classifier or pipeline, I'd prefer some plot over those grids. You not only learn the best parameters but also effects of classifier and feature number in a one plot. \n",
      "And this could be easily adopted to different number of paramters(instead of number of features) for classifiers that share a number of parameters in the optimization to see the effect of each parameter visually rather than getting the best parameters. \n",
      "This way you could both explore and gain insights about parameters that the classifier has."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could always pass another scoring function to compute the accuracies of each classifier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_model(X, y, scoring=metrics.f1_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_model(X, y, scoring=metrics.precision_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_model(X, y, scoring=metrics.recall_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluation Methods for Unsupervised Learning Algorithms\n",
      "- Talk about more on the evaluation functions that could be used to choose the number of clusters and number of classes for an unsupervised learning algorithm rather than choosing the model  from a number of parameters.\n",
      "- Silhouette score, elbow method for k-means; forother unsupervised learning algorithms, find other ways. Research!\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}