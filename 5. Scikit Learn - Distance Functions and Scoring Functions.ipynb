{
 "metadata": {
  "name": "",
  "signature": "sha256:359cb35f65963f32bcb99e633b04ae145e48a05b5ec3e6c861e0806a807ac2de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Distance and Scoring Functions\n",
      "If people asked me to name one thing that has been constantly ignored or not given importance in machine learning, first thing\n",
      "from top of my head would be evaluation. That is partially because some contexts(search, recommender systems) it is somehow difficult to build an evaluation function that first perfectly with the business logic. Partially, people end up measuring and optimizing towards wrong thing or not the _best_ thing. You are predicting the number of items for a distribution and most of the businesses that require these items are small businesses, so they require small number of items and may find difficult time to sell if the error is small when the prediction is not correct. For this application $l_2$(RMSE) may not make sense, because you will have errors in small quantitites and even if you are optimizing total error to be smallest, you are not optimizing for the business logic or the most beneficial state where it could be okay to error for large businessses in the quantity but somehow very small errors in small business. $l_1$ could be better evaluation distance function even if its error may exceed $l_2$. At this point, one needs to also think, maybe the evaluation function should not be the number of items to be sold, but rather $\\sum p(items * \\text{probability of an item is sold } |\\text{ business}_i)$. \n",
      "\n",
      "Of course, evaluation does not really mean anything without a proper evaluation function. If you are predicting $\\hat{y}$ and want to measure cost function from $y$, distance function that you will use is crucially important as it will determine the __model__. So, the distance function as well as what type of scoring function is using in the evaluation is quite important for how your model performs.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Distance Functions\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scoring Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Custom Metric Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.SCORERS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{'accuracy': make_scorer(accuracy_score),\n",
        " 'adjusted_rand_score': make_scorer(adjusted_rand_score),\n",
        " 'average_precision': make_scorer(average_precision_score, needs_threshold=True),\n",
        " 'f1': make_scorer(f1_score),\n",
        " 'log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
        " 'mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        " 'mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),\n",
        " 'precision': make_scorer(precision_score),\n",
        " 'r2': make_scorer(r2_score),\n",
        " 'recall': make_scorer(recall_score),\n",
        " 'roc_auc': make_scorer(roc_auc_score, needs_threshold=True)}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scoring_funcs = [ii for ii in dir(metrics) if 'score' in ii and not ii.startswith('_')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scoring_funcs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['accuracy_score',\n",
        " 'adjusted_mutual_info_score',\n",
        " 'adjusted_rand_score',\n",
        " 'auc_score',\n",
        " 'average_precision_score',\n",
        " 'completeness_score',\n",
        " 'consensus_score',\n",
        " 'explained_variance_score',\n",
        " 'f1_score',\n",
        " 'fbeta_score',\n",
        " 'homogeneity_score',\n",
        " 'jaccard_similarity_score',\n",
        " 'make_scorer',\n",
        " 'mutual_info_score',\n",
        " 'normalized_mutual_info_score',\n",
        " 'precision_recall_fscore_support',\n",
        " 'precision_score',\n",
        " 'r2_score',\n",
        " 'recall_score',\n",
        " 'roc_auc_score',\n",
        " 'scorer',\n",
        " 'silhouette_score',\n",
        " 'v_measure_score']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}